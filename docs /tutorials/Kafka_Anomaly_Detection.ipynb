{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiAznfsfCQzs4FeLdamrw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tvpian/Kafka-Stream-Processing/blob/main/docs%20/tutorials/Kafka_Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rodrigo-arenas/kafkaml-anomaly-detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UPhW9KL5gvW",
        "outputId": "697481dc-df6c-45ff-f63e-dc37a69761ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kafkaml-anomaly-detection'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 175 (delta 88), reused 134 (delta 55), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (175/175), 9.91 MiB | 22.35 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/kafkaml-anomaly-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geVbstFz5hTg",
        "outputId": "fecff1b4-dd60-4c77-cbdd-a58f757e4846"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kafkaml-anomaly-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeknordaagLf",
        "outputId": "beba0849-262f-4cbe-9d10-05c282ec5e70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/246.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/246.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.errors import KafkaError\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "zY4hrXZJGOXj",
        "outputId": "283f7d10-084a-467a-a219-38af64ae4a95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kafka'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1e0cf2aeba0a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKafkaConsumer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kafka'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -sSOL https://dlcdn.apache.org/kafka/3.5.2/kafka_2.13-3.5.2.tgz\n",
        "!tar -xzf kafka_2.13-3.5.2.tgz"
      ],
      "metadata": {
        "id": "Lh4h6pxlAqw0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.2/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.5.2/config/zookeeper.properties\n",
        "!./kafka_2.13-3.5.2/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.5.2/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvMXGAWYAsuC",
        "outputId": "ef90613a-271e-467a-8ce8-3e62eed75958"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep kafka\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJiYUQaLAyOz",
        "outputId": "d2f98119-341d-45b2-eb0c-c89b56361c37"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        8526     659  0 05:24 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root        8528    8526  0 05:24 ?        00:00:00 grep kafka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !./kafka_2.13-3.5.2/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic susy-train\n",
        "# !./kafka_2.13-3.5.2/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic susy-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqWD5XjUA1cV",
        "outputId": "532c15dd-97c5-4f3c-d011-1e43edeb8cda"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic susy-train.\n",
            "Created topic susy-test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.2/bin/kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --topic transactions --create --partitions 3 --replication-factor 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr2Miw7WBR7C",
        "outputId": "2a926656-6e2b-4d4b-8180-76fffbed5f62"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic transactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.2/bin/kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --topic anomalies --create --partitions 3 --replication-factor 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP1bmr88FeSs",
        "outputId": "628c1cc8-8d06-475f-daef-3fc03cabc492"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic anomalies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "# Generate train data\n",
        "X = 0.3 * rng.randn(500, 2)\n",
        "X_train = np.r_[X + 2, X - 2]\n",
        "X_train = np.round(X_train, 3)\n",
        "\n",
        "# fit the model\n",
        "clf = IsolationForest(n_estimators=50, max_samples=500, random_state=rng, contamination=0.01)\n",
        "clf.fit(X_train)\n",
        "\n",
        "dump(clf, './isolation_forest.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYvTpHb4BZq4",
        "outputId": "7aeb3de6-9bb9-4d55-c10b-3c84e5bd8d36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./isolation_forest.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))\n",
        "\n",
        "# write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "# write_to_kafka(\"susy-test\", zip(x_test, y_test))"
      ],
      "metadata": {
        "id": "6kmPluqSGCzG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSACTIONS_TOPIC = \"transactions\"\n",
        "OUTLIERS_GENERATION_PROBABILITY = 0.7\n",
        "DELAY = 0.01\n",
        "TRANSACTIONS_CONSUMER_GROUP = \"transactions\"\n",
        "NUM_PARTITIONS = 3\n",
        "ANOMALIES_TOPIC = \"anomalies\"\n",
        "ANOMALIES_CONSUMER_GROUP = \"anomalies\""
      ],
      "metadata": {
        "id": "1oSoozjeGsYt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_id = 0\n",
        "producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "\n",
        "if producer is not None:\n",
        "    while _id<1000:\n",
        "        # Generate some abnormal observations\n",
        "        if random.random() <= OUTLIERS_GENERATION_PROBABILITY:\n",
        "            X_test = np.random.uniform(low=-4, high=4, size=(1, 2)).tolist()\n",
        "        else:\n",
        "            X = 0.3 * np.random.randn(1, 2)\n",
        "            X_test = (X + np.random.choice(a=[2, -2], size=1, p=[0.5, 0.5]))\n",
        "            X_test = np.round(X_test, 3).tolist()\n",
        "\n",
        "        current_time = datetime.utcnow().isoformat()\n",
        "        print(record)\n",
        "        record = {\"id\": _id, \"data\": X_test, \"current_time\": current_time}\n",
        "        record = json.dumps(record).encode(\"utf-8\")\n",
        "\n",
        "        producer.send(topic=TRANSACTIONS_TOPIC,\n",
        "                         value=record,\n",
        "                         key=str(_id).encode('utf-8')).add_errback(error_callback)\n",
        "        producer.flush()\n",
        "        _id += 1\n",
        "        time.sleep(DELAY)"
      ],
      "metadata": {
        "id": "yZ9fzVAjFmpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.13-3.5.2/bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic transactions  --from-beginning --max-messages 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUVrzhG2HCiU",
        "outputId": "92cde968-c0a7-4192-ace0-11c6dd7931e7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator:Heartbeat failed for group transactions because it is rebalancing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\": 4, \"data\": [[0.3144653901201675, 2.5618138956780223]], \"current_time\": \"2024-04-27T05:53:07.529469\"}\n",
            "{\"id\": 6, \"data\": [[3.3512449865829463, 3.7689977665826184]], \"current_time\": \"2024-04-27T05:53:07.554834\"}\n",
            "{\"id\": 10, \"data\": [[2.058, 2.012]], \"current_time\": \"2024-04-27T05:53:07.604694\"}\n",
            "{\"id\": 12, \"data\": [[-2.305173790219178, -2.9625789012955517]], \"current_time\": \"2024-04-27T05:53:07.630644\"}\n",
            "{\"id\": 13, \"data\": [[1.2079448395365233, -1.0658623090674242]], \"current_time\": \"2024-04-27T05:53:07.642850\"}\n",
            "{\"id\": 14, \"data\": [[2.264618535639734, 3.104725499305074]], \"current_time\": \"2024-04-27T05:53:07.655069\"}\n",
            "{\"id\": 18, \"data\": [[0.2997946606215729, 0.40751176649028586]], \"current_time\": \"2024-04-27T05:53:07.704825\"}\n",
            "{\"id\": 19, \"data\": [[-2.0666015252476093, 0.19319531374695487]], \"current_time\": \"2024-04-27T05:53:07.717326\"}\n",
            "{\"id\": 20, \"data\": [[-2.2373495236108365, -0.6640118717547212]], \"current_time\": \"2024-04-27T05:53:07.729931\"}\n",
            "{\"id\": 24, \"data\": [[2.772489206305634, -1.1769559872680961]], \"current_time\": \"2024-04-27T05:53:07.779916\"}\n",
            "Processed a total of 10 messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from joblib import load\n",
        "import logging\n",
        "from multiprocessing import Process\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model_path = os.path.abspath('/content/isolation_forest.joblib')\n",
        "clf = load(model_path)\n",
        "consumer = KafkaConsumer(TRANSACTIONS_TOPIC, bootstrap_servers=['127.0.0.1:9092'],\n",
        "                          group_id = TRANSACTIONS_CONSUMER_GROUP, auto_offset_reset='earliest',\n",
        "  enable_auto_commit=True)\n",
        "\n",
        "producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
        "\n",
        "def detect():\n",
        "\n",
        "\n",
        "    while True:\n",
        "        message = consumer.poll()\n",
        "        if message is None:\n",
        "            continue\n",
        "        # if message.error():\n",
        "        #     logging.error(\"Consumer error: {}\".format(message.error()))\n",
        "        #     continue\n",
        "        print(message)\n",
        "        # Message that came from producer\n",
        "        record = json.loads(message.value().decode('utf-8'))\n",
        "        data = record[\"data\"]\n",
        "\n",
        "        prediction = clf.predict(data)\n",
        "\n",
        "        # If an anomaly comes in, send it to anomalies topic\n",
        "        if prediction[0] == -1:\n",
        "            score = clf.score_samples(data)\n",
        "            record[\"score\"] = np.round(score, 3).tolist()\n",
        "\n",
        "            _id = str(record[\"id\"])\n",
        "            record = json.dumps(record).encode(\"utf-8\")\n",
        "\n",
        "            producer.produce(topic=ANOMALIES_TOPIC,\n",
        "                             value=record,\n",
        "                             key=_id)\n",
        "            producer.flush()\n",
        "\n",
        "    consumer.close()\n",
        "\n",
        "# Call the detect function\n",
        "detect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "KydGg2QZI7XF",
        "outputId": "df350ed9-313f-45ac-bee0-87f459217ddf"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator:Heartbeat failed for group transactions because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group transactions because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group transactions because it is rebalancing\n",
            "WARNING:kafka.coordinator:Heartbeat failed for group transactions because it is rebalancing\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-c394adb06c22>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Call the detect function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-c394adb06c22>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMap\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0mto\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \"\"\"\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# Fetch positions if we have partitions we're subscribed to that we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/coordinator/consumer.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_active_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/coordinator/base.py\u001b[0m in \u001b[0;36mensure_active_group\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msucceeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# avoid negative timeouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;31m# called without the lock to avoid deadlock potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_kafka_item(item):\n",
        "  print(item)\n",
        "\n",
        "BATCH_SIZE=64\n",
        "SHUFFLE_BUFFER_SIZE=64\n",
        "train_ds = tfio.IODataset.from_kafka('transactions', partition=0, offset=0)\n",
        "train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "decode_kafka_item\n"
      ],
      "metadata": {
        "id": "d7ycgbmUXNzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "consumer = KafkaConsumer(\n",
        "     TRANSACTIONS_TOPIC,\n",
        "     bootstrap_servers=['127.0.0.1:9092'],\n",
        "     auto_offset_reset='earliest',\n",
        "     enable_auto_commit=True,\n",
        "     group_id='my-group',\n",
        "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
      ],
      "metadata": {
        "id": "kObcYM-2YGU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from json import loads\n",
        "consumer = KafkaConsumer(TRANSACTIONS_TOPIC, bootstrap_servers=['127.0.0.1:9092'],\n",
        "                          group_id = TRANSACTIONS_CONSUMER_GROUP, auto_offset_reset='earliest',\n",
        "  enable_auto_commit=True, value_deserializer=lambda x: loads(x.decode('utf-8')))"
      ],
      "metadata": {
        "id": "QIzlkjNKZQiK"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in consumer:\n",
        "    message = message.value\n",
        "    collection.insert_one(message)\n",
        "    print('{} added to {}'.format(message, collection))"
      ],
      "metadata": {
        "id": "b5liLHrgZkIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}